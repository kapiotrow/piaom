{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 5: Segmentacja polipów — Architektura typu U-Net(Kvasir-SEG)\n",
    "\n",
    "W tym ćwiczeniu chcemy nauczyć się budowania i trenowania modeli **segmentacji semantycznej** z wykorzystaniem architektury **U-Net** oraz frameworka **PyTorch Lightning** na zbiorze danych medycznych **Kvasir-SEG** (segmentacja polipów jelitowych w obrazach endoskopowych).\n",
    "\n",
    "**Główne zagadnienia:**\n",
    "- Implementacja `LightningDataModule`: podział train/val/test, augmentacje spójne dla obrazu i maski.\n",
    "- Budowa architektury **U-Net 2D** z enkoderem i dekoderem oraz skip connections.\n",
    "- Funkcje straty dla segmentacji: **BCEWithLogitsLoss** i **Dice Loss**.\n",
    "- Metryki: **Dice Score** i **IoU** (Intersection over Union) z biblioteki torchmetrics.\n",
    "- Wykorzystanie **Transfer Learning** ResNet jako encoder w U-Net.\n",
    "\n",
    "**Dataset:** \n",
    "Użyjemy **Kvasir-SEG** – zbioru zawierającego obrazy endoskopowe jelita grubego wraz z maskami segmentacji polipów. Dataset ten został stworzony do oceny algorytmów segmentacji w endoskopii i zawiera około 1000 par obraz-maska. Dane zostaną **automatycznie pobrane** w notebooku. Skalujemy je również do rozdzielczości 224x224 aby ograniczyć czas potrzebny na uczenie sieci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Instalacja i importy\n",
    "\n",
    "Jeśli chcemy powtarzalnych wyników możemy ustawić stały SEED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b032856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | Lightning: 2.5.5\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install pytorch-lightning torchmetrics scikit-image opencv-python pillow --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import os, zipfile, urllib.request, random, glob, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "\n",
    "# SEED = 42\n",
    "SEED = None\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "if SEED is not None:\n",
    "    set_seed()\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device, '| Lightning:', pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead66112",
   "metadata": {},
   "source": [
    "## 1) Pobranie i przygotowanie Kvasir-SEG\n",
    "\n",
    "**Kvasir-SEG** to zbiór danych zawierający obrazy endoskopowe jelita grubego wraz z maskami segmentacji polipów. Poniższy fragment automatycznie pobiera zbiór i sprawdza liczbę pobranych plików. \n",
    "\n",
    "Należy zwrócić uwagę, że dla zadania segmentacji zarówno obrazy wejściowe jak i groundtruth są obrazami. W przypadku segmentacji binarnej (tło-obiekt) maska ma wartości binarne. Wymusza to delikatnie inne podejście do zarządzania i przetwarzania wstępnego takich danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158b881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane już dostępne: data/data_kvasir/Kvasir-SEG\n",
      "Liczba obrazów/masek: 1000 1000\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('./data/data_kvasir')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ZIP_URL = 'https://datasets.simula.no/downloads/kvasir-seg.zip'\n",
    "ZIP_PATH = DATA_ROOT / 'kvasir.zip'\n",
    "EXTRACT_DIR = DATA_ROOT / 'Kvasir-SEG'\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    print('Pobieranie Kvasir-SEG (~150 MB)…')\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    urllib.request.urlretrieve(ZIP_URL, ZIP_PATH)\n",
    "    print('Rozpakowywanie…')\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
    "        zf.extractall(DATA_ROOT)\n",
    "    print('Gotowe:', EXTRACT_DIR)\n",
    "else:\n",
    "    print('Dane już dostępne:', EXTRACT_DIR)\n",
    "\n",
    "IMG_DIR = EXTRACT_DIR / 'images'\n",
    "MSK_DIR = EXTRACT_DIR / 'masks'\n",
    "imgs = sorted(glob.glob(str(IMG_DIR / '*.jpg')))\n",
    "msks = sorted(glob.glob(str(MSK_DIR / '*.jpg')))\n",
    "print('Liczba obrazów/masek:', len(imgs), len(msks))\n",
    "assert len(imgs) == len(msks) and len(imgs) > 0, 'Brak danych lub brak par obraz–maska.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e0f9f",
   "metadata": {},
   "source": [
    "## Zadanie 1 – Dataset i DataModule dla segmentacji\n",
    "\n",
    "W tym zadaniu zaimplementujesz własny Dataset oraz DataModule dla danych segmentacyjnych. Kluczową różnicą w porównaniu do klasyfikacji jest to, że augmentacje zmieniające pozycję obiektów (wszystkie przekształcenia geometryczne) muszą być stosowane zarówno do obrazu jak i maski – np. jeśli obracamy obraz o 10 stopni, to maskę również musimy obrócić o dokładnie ten sam kąt.\n",
    "\n",
    "**Dataset dla segmentacji:**\n",
    "1. Zaimplementuj klasę `KvasirDataset`, która dziedziczy po `Dataset`.\n",
    "2. W metodzie `__init__` zapamiętaj ścieżki do obrazów (`img_paths`) i masek (`msk_paths`), oraz opcjonalny obiekt transform.\n",
    "3. W metodzie `__len__` zwróć liczbę obrazów.\n",
    "4. W metodzie `__getitem__`:\n",
    "   - Wybierz odpowiednie ścieżki z zapamiętanych list podczas inicjalizacji.\n",
    "   - Wczytaj obraz i maskę używając funkcji pomocniczej `load_pair`.\n",
    "   - Jeśli `self.transform` istnieje, zastosuj go do obu (obraz, maska).\n",
    "   - Zwróć parę (obraz, maska).\n",
    "\n",
    "**Transform dla segmentacji:**\n",
    "1. Zaimplementuj klasę `SegmentationTransform`, która będzie stosować te same transformacje do obrazu i maski. W przypadku segmentacji musimy sami ją zaimplementować, aby móć stosować spójną segmentację dla obu obrazów.\n",
    "2. W metodzie `__init__`:\n",
    "   - Zapamiętaj `size` (docelowy rozmiar obrazu).\n",
    "   - Zapamiętaj `augment` (flaga czy stosować augmentacje, domyślnie False).\n",
    "   - Stwórz obiekt do konwersji PIL do tensor: `transforms.ToTensor()`.\n",
    "   - Stwórz **dwa** obiekty do resize: `transforms.Resize(size, ...)`. Pierwszy z interpolacją BILINEAR dla obrazu, a drugi z interpolacją NEAREST dla maski. Jest to ważne, gdyż maska jest binarna i nie chcemy rozmywać jej krawędzi.\n",
    "   - Stwórz obiekt normalizacji: `transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])` – przesunie wartości z [0,1] do [-1,1].\n",
    "3. W metodzie `__call__(self, img, mask)`:\n",
    "   - Wykonaj resize dla obrazu i maski (na obrazach PIL).\n",
    "   - Skonwertuj oba do tensorów (wartości [0, 1]).\n",
    "   - Zbinaryzuj maskę: `mask = (mask > 0).float()` – chcemy, żeby maska zawiera tylko 0 i 1, ale miała typ zmiennoprzecinkowy.\n",
    "   - Jeśli `self.augment == True`:\n",
    "     - Z prawdopodobieństwem 50% wykonaj horizontal flip (dla obu).\n",
    "     - Z prawdopodobieństwem 50% wykonaj losową rotację o kąt z przedziału [-10, 10] stopni. Zrób to samo dla obu obrazów, ale z różną interpolacją. Wykorzystaj `transforms.functional.rotate` i podaj argument `interpolation`.\n",
    "   - Znormalizuj obraz (nie maskę) do [-1, 1].\n",
    "   - Zwróć `(img.float(), mask.float())`.\n",
    "\n",
    "**DataModule:**\n",
    "1. Zaimplementuj klasę `KvasirDataModule`, która dziedziczy po `LightningDataModule`.\n",
    "2. W metodzie `__init__`:\n",
    "   - Zapamiętaj listy obrazów i masek.\n",
    "   - Zapamiętaj `batch`, `nw` (num_workers), `train_split`, `val_split`, `img_size`.\n",
    "   - Stwórz dwa obiekty transformacji: `self.train_transform` (z augment=True) i `self.val_test_transform` (z augment=False) jako obiekty zaimplementowanej wcześniej klasy.\n",
    "3. W metodzie `setup(self, stage=None)`:\n",
    "   - Wylosuj indeksy i podziel dane na train/val/test zgodnie z `train_split` i `val_split`. W tym celu stwórz listę indeksów od 0 do liczby próbek, a następnie wymieszaj je za pomocą `np.random.shuffle`. Podziel wynikowy wektor na 3 częsci zgodnie za przekazanym podziałem na część treningową, walidacyjną i testową. Następnie podziel również odpowiednio ścieżki do plików zgodnie z przygotowanymi listami indeksów.\n",
    "   - Stwórz i zapamiętaj trzy datasety (treningowy, walidacyjny i testowy) z odpowiednimi transformacjami i zawierające próbki zgodnie w wcześniejszym losowaniem.\n",
    "   - Wyświetl rozmiary zbiorów.\n",
    "4. Zaimplementuj metody `train_dataloader()`, `val_dataloader()`, `test_dataloader()`:\n",
    "   - Zwróć DataLoader z odpowiednim datastem.\n",
    "   - Ustaw `shuffle=True` tylko dla train.\n",
    "   - Ustaw `pin_memory=True` jeśli CUDA dostępne.\n",
    "\n",
    "**Poza klasami:**\n",
    "1. Stwórz instancję `KvasirDataModule` z listami `imgs` i `msks` (były stworzone we wcześniejszej komórce), rozmiarem batcha i liczbą workerów. Wartości te trzeba dobrać na podstawie specyfikacji maszyny na której uruchamiany będzie trening.\n",
    "2. Wywołaj `dm.setup()`.\n",
    "3. Pobierz jeden batch z train loadera (`next(iter(dm.train_dataloader()))`) i wyświetl kształty. Powinny pojawić się kształty typu `(16, 3, 384, 384)` dla obrazów i `(16, 1, 384, 384)` dla masek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c39e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiary zbiorów: Train: 700, Val: 150, Test: 150\n",
      "Kształt obrazów: torch.Size([16, 3, 384, 384])\n",
      "Kształt masek: torch.Size([16, 1, 384, 384])\n",
      "Zakres wartości obrazów: [-1.000, 1.000]\n",
      "Unikalne wartości w maskach: tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, img_paths, msk_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.msk_paths = msk_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        msk_path = self.msk_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(msk_path).convert('L')\n",
    "        if self.transform:\n",
    "            img, mask = self.transform(img, mask)\n",
    "        else:\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            img = to_tensor(img).float()\n",
    "            mask = to_tensor(mask).float()\n",
    "            mask = (mask > 0).float()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "class SegmentationTransform:\n",
    "    def __init__(self, size=(384, 384), augment=False):\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.img_resize = transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        self.mask_resize = transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        img = self.img_resize(img)\n",
    "        mask = self.mask_resize(mask)\n",
    "        img = self.to_tensor(img)\n",
    "        mask = self.to_tensor(mask)\n",
    "        mask = (mask > 0).float()\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "                mask = transforms.functional.hflip(mask)\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                img = transforms.functional.rotate(\n",
    "                    img, angle, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "                )\n",
    "                mask = transforms.functional.rotate(\n",
    "                    mask, angle, interpolation=transforms.InterpolationMode.NEAREST\n",
    "                )\n",
    "        \n",
    "        img = self.normalize(img)\n",
    "        \n",
    "        return img.float(), mask.float()\n",
    "\n",
    "class KvasirDataModule(LightningDataModule):\n",
    "    def __init__(self, img_paths, msk_paths, batch_size=16, num_workers=2, \n",
    "                 train_split=0.7, val_split=0.15, img_size=(384, 384)):\n",
    "        super().__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.msk_paths = msk_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_split = train_split\n",
    "        self.val_split = val_split\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.train_transform = SegmentationTransform(size=img_size, augment=True)\n",
    "        self.val_test_transform = SegmentationTransform(size=img_size, augment=False)\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        n_samples = len(self.img_paths)\n",
    "        indices = list(range(n_samples))\n",
    "        \n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_end = int(self.train_split * n_samples)\n",
    "        val_end = train_end + int(self.val_split * n_samples)\n",
    "\n",
    "        train_indices = indices[:train_end]\n",
    "        val_indices = indices[train_end:val_end]\n",
    "        test_indices = indices[val_end:]\n",
    "\n",
    "        train_imgs = [self.img_paths[i] for i in train_indices]\n",
    "        train_msks = [self.msk_paths[i] for i in train_indices]\n",
    "        \n",
    "        val_imgs = [self.img_paths[i] for i in val_indices]\n",
    "        val_msks = [self.msk_paths[i] for i in val_indices]\n",
    "        \n",
    "        test_imgs = [self.img_paths[i] for i in test_indices]\n",
    "        test_msks = [self.msk_paths[i] for i in test_indices]\n",
    "        \n",
    "        self.train_dataset = KvasirDataset(train_imgs, train_msks, self.train_transform)\n",
    "        self.val_dataset = KvasirDataset(val_imgs, val_msks, self.val_test_transform)\n",
    "        self.test_dataset = KvasirDataset(test_imgs, test_msks, self.val_test_transform)\n",
    "        \n",
    "        print(f\"Rozmiary zbiorów: Train: {len(self.train_dataset)}, \"\n",
    "              f\"Val: {len(self.val_dataset)}, Test: {len(self.test_dataset)}\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "\n",
    "dm = KvasirDataModule(\n",
    "    img_paths=imgs,\n",
    "    msk_paths=msks,\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    img_size=(384, 384)\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "batch_imgs, batch_masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Kształt obrazów: {batch_imgs.shape}\")\n",
    "print(f\"Kształt masek: {batch_masks.shape}\")\n",
    "print(f\"Zakres wartości obrazów: [{batch_imgs.min():.3f}, {batch_imgs.max():.3f}]\")\n",
    "print(f\"Unikalne wartości w maskach: {torch.unique(batch_masks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354f6d9",
   "metadata": {},
   "source": [
    "## Zadanie 2 – Implementacja U-Net: architektura enkodera i dekodera\n",
    "\n",
    "W tym zadaniu zaimplementujesz architekturę **U-Net** – jedną z najpopularniejszych sieci do segmentacji medycznej. U-Net składa się z:\n",
    "- **Enkodera** (downsampling path): ekstraktuje cechy wysokiego poziomu, zmniejszając rozdzielczość.\n",
    "- **Dekodera** (upsampling path): odbudowuje rozdzielczość, tworząc maskę segmentacji.\n",
    "- **Skip connections**: połączenia z enkodera do dekodera na tym samym poziomie rozdzielczości – pozwalają zachować szczegóły przestrzenne.\n",
    "\n",
    "![U-Net architecture diagram](https://iq.opengenus.org/content/images/2021/12/1_ovEGmOI3bcCeauu8jEBzsg.png)\n",
    "\n",
    "**Blok DoubleConv:**\n",
    "1. Zaimplementuj klasę `DoubleConv`, która dziedziczy po `nn.Module`.\n",
    "2. Przyjmuje parametry: `in_ch` (kanały wejściowe), `out_ch` (kanały wyjściowe), `dropout` (prawdopodobieństwo dropout, domyślnie 0.0).\n",
    "Dropout to technika regularyzacji, która losowo \"wyłącza\" pewien procent neuronów podczas treningu. Zmusza to sieć do uczenia się bardziej uniwersalnych i odpornych reprezentacji, ponieważ nie może polegać na pojedynczych neuronach/kanałach. Będziemy wykorzystywać `Dropout2d`, który usuwa całe kanały w warstwach konwolucyjnych. Warstwy te pozwalają ograniczyć zjawisko przeuczenia (overfitting).\n",
    "3. Zbuduj sekwencję warstw:\n",
    "   - `Conv2d(in_ch, out_ch, kernel_size=3, padding=1)` – konwolucja bez zmiany rozmiaru.\n",
    "   - `BatchNorm2d(out_ch)` – normalizacja batch.\n",
    "   - `ReLU(inplace=True)` – aktywacja.\n",
    "   - Opcjonalnie: `Dropout2d(p=dropout)` jeśli dropout > 0.\n",
    "   - Powtórz: `Conv2d(out_ch, out_ch, 3, padding=1)`, `BatchNorm2d`, `ReLU`. Ewentualnie może tutaj również nastąpić zmiana liczby kanałów wyjściowych. Wtedy na wejściu podczas inicjalizacji musimy przekazać dodatkowy parametr.\n",
    "   - Opcjonalnie: ponownie `Dropout2d(p=dropout)` jeśli dropout > 0.\n",
    "4. W metodzie `forward(x)` przepuść dane przez zbudowaną sekwencję.\n",
    "\n",
    "**Model UNetSmall:**\n",
    "1. Zaimplementuj klasę `UNetSmall`, która dziedziczy po `nn.Module`.\n",
    "2. W `__init__` przyjmij: `in_ch1=3` (RGB), `out_ch=1` (maska binarna), `dropout=0.0`.\n",
    "3. **Encoder (ścieżka downsampling):**\n",
    "   - `DoubleConv(in_ch1, out_ch1, dropout)` – pierwszy blok (d1).\n",
    "   - `nn.MaxPool2d(2)` – pooling 2×2 (zmniejsza rozdzielczość o połowę) (p1).\n",
    "   - `DoubleConv(out_ch1, out_ch2, dropout)` – drugi blok (d2).\n",
    "   - `nn.MaxPool2d(2)` (p2).\n",
    "   - `DoubleConv(out_ch2, out_ch3, dropout)` – trzeci blok (d3).\n",
    "   - `nn.MaxPool2d(2)` (p3).\n",
    "4. **Bottleneck (najgłębsza część):**\n",
    "   - `DoubleConv(out_ch3, out_ch4, dropout)` – ewentualnie w tej warstwie można zwiększyć dropout względem innych warstw (b).\n",
    "5. **Decoder (ścieżka upsampling):**\n",
    "   - `nn.ConvTranspose2d(out_ch4, out_ch3, kernel_size=2, stride=2)` – upsampling (zwiększa rozdzielczość 2×) (u3).\n",
    "   - `DoubleConv(2*out_ch3, out_ch3, dropout)` – uwaga: 2*out_ch3 = out_ch3 (z poprzedniej warstwy) + out_ch3 (skip connection z enkodera) (c3).\n",
    "   - `nn.ConvTranspose2d(out_ch3, out_ch2, 2, 2)` (u2).\n",
    "   - `DoubleConv(2*out_ch2, out_ch2, dropout)` (c2).\n",
    "   - `nn.ConvTranspose2d(out_ch2, out_ch1, 2, 2)` (u1).\n",
    "   - `DoubleConv(2*out_ch1, out_ch1, dropout)` (c1).\n",
    "6. **Warstwa wyjściowa:**\n",
    "   - `nn.Conv2d(out_ch1, out_ch, kernel_size=1)` – konwolucja 1×1, zwraca logity (out).\n",
    "7. W metodzie `forward(x)`:\n",
    "   - **Encoder:** przepuść wejście przez `d1→p1→d2→p2→d3→p3→b`, zapamiętuj wyjścia z d1, d2 i d3, bo są potrzebne do skip connections.\n",
    "   - **Decoder:**\n",
    "     - Przepuść wyjście z bottleneck przez u3, potem skonkatenuj ze skip connection z d3 `torch.cat([u3, d3], dim=1)`, a następnie przepuść przez c3.\n",
    "     - Zrób do samo wykorzystując warstwy u2 i c2 oraz wyjście z d2.\n",
    "     - Zrób do samo wykorzystując warstwy u1 i c1 oraz wyjście z d1.\n",
    "   - Na koniec przepuść dane przez warstwę wyjściową out.\n",
    "\n",
    "**Dice Loss:**\n",
    "1. Zaimplementuj klasę `DiceLoss`, dziedziczącą po `nn.Module`.\n",
    "2. W `__init__(self, eps=1e-6)` zapamiętaj `eps=1e-6` (wartość dodawana dla stabilności numerycznej).\n",
    "3. DiceLoss pochodzi od współczynnika Dice (Dice coefficient, F1 dla segmentacji), który mierzy nakładanie się dwóch masek: `Dice = 2 * |P ∩ G| / (|P| + |G|)`, gdzie `P` to predykcja, `G` to maska, natomiast operator | | oznacza liczność zbioru. Zakres wartości to [0, 1], im większe — tym lepsze dopasowanie. `DiceLoss` wynosi `1 − Dice` (dla zadanie minimalizacji). Dodajemy małą wartość eps dla stabilności numerycznej do licznika i mianownika. Skupia się on na nakładaniu się obszarów, a nie na pojedynczych pikselach, co daje lepszą jakość predykcji maski.\n",
    "4. W `forward(self, logits, targets)`:\n",
    "   - Zastosuj sigmoid do logitów: `torch.sigmoid(logits)`.\n",
    "   - Upewnij się że targets są typu float.\n",
    "   - Oblicz licznik. W tym celu pomnóż wyniki sigmoidy z `targets`, a następnie zsumuj po wymiarach przestrzennych, pomnóż przez 2 i dodał `eps`.\n",
    "   - Oblicz mianownik symetrycznie do licznika, zgodnie z wzorem.\n",
    "   - Podziel otrzymane wartości, oblicz wartość średnią po wszystkich wymiarach (batch) i odejmij wynik od 1. Zwróć obliczoną wielkość.\n",
    "\n",
    "**Callbacks i Lightning Module:**\n",
    "1. Kod zawiera już gotową implementację `MetricsCallback` (do zbierania metryk).\n",
    "2. Konieczna jest jeszcze implementacja `LitUNet` (Lightning wrapper dla zaprejektowanej sieci z optymalizatorem, stratami i metrykami), która dziedziczy po `LightningModule`. Implementacja jest podobna do poprzedniego ćwiczenia.\n",
    "3. W metodzie `__init__(self, in_ch=3, lr=1e-3, dropout=0.0)`:\n",
    "   - Wywołaj `super().__init__()`.\n",
    "   - Zapisz hiperparametry: `self.save_hyperparameters()` – Lightning automatycznie zapisze je w checkpointach.\n",
    "   - Stwórz instancję stworzonej wcześniej sieci.\n",
    "   - Zdefiniuj i zapamiętaj funkcje straty:\n",
    "     - `nn.BCEWithLogitsLoss()` – Binary Cross-Entropy (działa na logitach, wewnętrznie aplikuje sigmoid).\n",
    "     - `DiceLoss()` – nasza implementacja Dice Loss.\n",
    "   - Zdefiniuj metryki (z torchmetrics):\n",
    "     - `BinaryJaccardIndex()` – IoU (Intersection over Union).\n",
    "     - `BinaryF1Score()` – F1/Dice Score. Odpowiednik zaimplementowanej fukncji straty, ale używa progowania prawdopodobieństwa, przez co nie można dla niej obliczyć gradientu.\n",
    "4. W metodzie `forward(self, x)`:\n",
    "   - Przepuść wejście przez sieć.\n",
    "5. W metodzie `configure_optimizers(self)`:\n",
    "   - Wykorzystaj optymalizator `Adam`.\n",
    "   - Wykorzystaj scheduler `CosineAnnealingLR`.\n",
    "   - Zwróć słownik zawierający optymalizator i scheduler.\n",
    "6. Zdefiniuj metody `training_step`, `validation_step` i `test_step`. Prawie wszystkie operacje są w nich takie sameg więc można je przenieść do funkcji pomocniczej.\n",
    "   - Rozpakuj batch (obrazy i maski).\n",
    "   - Oblicz wyjście sieci `self(x)`.\n",
    "   - Oblicz stratę jako sumę BCEWithLogitsLoss + DiceLoss\n",
    "   - Oblicz metryki do monitoringu w bloku `with torch.no_grad():`:\n",
    "     - Wykonaj progowanie predykcji sieci, aby wyznaczyć wynik segmentacji.\n",
    "     - Upewnij się że zarówno maska groundtruth i wynik predykcji są typu `int`.\n",
    "     - Oblicz IoU i F1(Dice) za pomocą stworzonych podczas inicjalizacji obiektów.\n",
    "   - Zaloguj metryki za pomocą funkcji `self.log()`:\n",
    "     - Pierwszym argumentem jest nazwa logowanej metryki.\n",
    "     - Drugim argumentem jest jej wartość.\n",
    "     - Flaga `on_epoch` sprawia, że metryka jest agregowana i logowana dla całej epoki. Ustaw ją jako `True`.\n",
    "     - Flaga `prog_bar` mówi czy wartość tej metryki ma być wyświetlona obok paska postępu.\n",
    "   - Zwróć `loss` dla `training_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb98b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(pl.Callback):\n",
    "    \"\"\"Callback to collect training metrics for visualization\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_dice': [],\n",
    "            'val_dice': [],\n",
    "            'train_iou': [],\n",
    "            'val_iou': [],\n",
    "            'epoch': []\n",
    "        }\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Collect training metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        epoch = trainer.current_epoch + 1\n",
    "        \n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        self.metrics['train_loss'].append(metrics.get('train_loss_epoch', float('nan')).item())\n",
    "        self.metrics['train_dice'].append(metrics.get('train_dice_epoch', float('nan')).item())\n",
    "        self.metrics['train_iou'].append(metrics.get('train_iou_epoch', float('nan')).item())\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Collect validation metrics\n",
    "        metrics = trainer.callback_metrics\n",
    "        \n",
    "        # Only add validation metrics if we have them\n",
    "        if 'val_loss' in metrics:\n",
    "            self.metrics['val_loss'].append(metrics['val_loss'].item())\n",
    "            self.metrics['val_dice'].append(metrics['val_dice'].item())\n",
    "            self.metrics['val_iou'].append(metrics['val_iou'].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fc9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unikalne wartości w y_test: tensor([0., 1.])\n",
      "Input shape: torch.Size([2, 3, 384, 384])\n",
      "Output shape: torch.Size([2, 1, 384, 384])\n",
      "Dice loss: 0.5064\n",
      "Optymalizator i scheduler skonfigurowane poprawnie\n",
      "Training step loss: 1.1849\n",
      "MetricsCallback zainicjalizowany poprawnie\n",
      "Wszystkie testy zakończone pomyślnie!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolina/studia/piaom/venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:449: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "# Blok DoubleConv\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout2d(p=dropout))\n",
    "            \n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout2d(p=dropout))\n",
    "            \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Model UNetSmall\n",
    "class UNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        out_ch1 = 64\n",
    "        out_ch2 = 128\n",
    "        out_ch3 = 256\n",
    "        out_ch4 = 512\n",
    "        \n",
    "        # Encoder\n",
    "        self.d1 = DoubleConv(in_ch, out_ch1, dropout)\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.d2 = DoubleConv(out_ch1, out_ch2, dropout)\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.d3 = DoubleConv(out_ch2, out_ch3, dropout)\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.b = DoubleConv(out_ch3, out_ch4, dropout)\n",
    "        \n",
    "        # Decoder\n",
    "        self.u3 = nn.ConvTranspose2d(out_ch4, out_ch3, kernel_size=2, stride=2)\n",
    "        self.c3 = DoubleConv(2 * out_ch3, out_ch3, dropout)\n",
    "        \n",
    "        self.u2 = nn.ConvTranspose2d(out_ch3, out_ch2, kernel_size=2, stride=2)\n",
    "        self.c2 = DoubleConv(2 * out_ch2, out_ch2, dropout)\n",
    "        \n",
    "        self.u1 = nn.ConvTranspose2d(out_ch2, out_ch1, kernel_size=2, stride=2)\n",
    "        self.c1 = DoubleConv(2 * out_ch1, out_ch1, dropout)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(out_ch1, out_ch, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1_out = self.d1(x)\n",
    "        p1_out = self.p1(d1_out)\n",
    "        \n",
    "        d2_out = self.d2(p1_out)\n",
    "        p2_out = self.p2(d2_out)\n",
    "        \n",
    "        d3_out = self.d3(p2_out)\n",
    "        p3_out = self.p3(d3_out)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b_out = self.b(p3_out)\n",
    "        \n",
    "        # Decoder\n",
    "        u3_out = self.u3(b_out)\n",
    "        c3_in = torch.cat([u3_out, d3_out], dim=1)\n",
    "        c3_out = self.c3(c3_in)\n",
    "        \n",
    "        u2_out = self.u2(c3_out)\n",
    "        c2_in = torch.cat([u2_out, d2_out], dim=1)\n",
    "        c2_out = self.c2(c2_in)\n",
    "        \n",
    "        u1_out = self.u1(c2_out)\n",
    "        c1_in = torch.cat([u1_out, d1_out], dim=1)\n",
    "        c1_out = self.c1(c1_in)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(c1_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        probs_flat = probs.view(probs.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "        intersection = 2 * (probs_flat * targets_flat).sum(dim=1)\n",
    "        union = probs_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "        dice = (intersection + self.eps) / (union + self.eps)\n",
    "        \n",
    "        return 1 - dice.mean()\n",
    "\n",
    "# Lightning Module dla U-Net\n",
    "# Lightning Module dla U-Net - POPRAWIONA WERSJA\n",
    "class LitUNet(LightningModule):\n",
    "    def __init__(self, in_ch=3, lr=1e-3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = UNetSmall(in_ch=in_ch, out_ch=1, dropout=dropout)\n",
    "        \n",
    "        # Funkcje straty\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        \n",
    "        # Metryki - używamy _epoch do agregacji\n",
    "        self.train_iou = BinaryJaccardIndex()\n",
    "        self.val_iou = BinaryJaccardIndex()\n",
    "        self.test_iou = BinaryJaccardIndex()\n",
    "        \n",
    "        self.train_f1 = BinaryF1Score()\n",
    "        self.val_f1 = BinaryF1Score()\n",
    "        self.test_f1 = BinaryF1Score()\n",
    "        \n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=10, eta_min=1e-6\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler\n",
    "        }\n",
    "    \n",
    "    def _shared_step(self, batch, batch_idx, prefix):\n",
    "        x, y_true = batch\n",
    "        \n",
    "        # Predykcja\n",
    "        y_pred = self(x)\n",
    "        \n",
    "        # Obliczanie strat\n",
    "        bce_loss = self.bce_loss(y_pred, y_true)\n",
    "        dice_loss = self.dice_loss(y_pred, y_true)\n",
    "        total_loss = bce_loss + dice_loss\n",
    "        \n",
    "        # Obliczanie metryk\n",
    "        with torch.no_grad():\n",
    "            y_pred_bin = (torch.sigmoid(y_pred) > 0.5).float()\n",
    "            y_true_int = y_true.int()\n",
    "            y_pred_int = y_pred_bin.int()\n",
    "            \n",
    "            if prefix == 'train':\n",
    "                iou = self.train_iou(y_pred_int, y_true_int)\n",
    "                f1 = self.train_f1(y_pred_int, y_true_int)\n",
    "            elif prefix == 'val':\n",
    "                iou = self.val_iou(y_pred_int, y_true_int)\n",
    "                f1 = self.val_f1(y_pred_int, y_true_int)\n",
    "            else:  # test\n",
    "                iou = self.test_iou(y_pred_int, y_true_int)\n",
    "                f1 = self.test_f1(y_pred_int, y_true_int)\n",
    "        \n",
    "        # Logowanie - POPRAWIONE dla zgodności z MetricsCallback\n",
    "        self.log(f'{prefix}_loss', total_loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{prefix}_loss_epoch', total_loss, on_epoch=True, prog_bar=False)\n",
    "        self.log(f'{prefix}_bce_loss', bce_loss, on_epoch=True)\n",
    "        self.log(f'{prefix}_dice_loss', dice_loss, on_epoch=True)\n",
    "        \n",
    "        # Logowanie F1 jako dice dla zgodności z MetricsCallback\n",
    "        self.log(f'{prefix}_dice', f1, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{prefix}_dice_epoch', f1, on_epoch=True, prog_bar=False)\n",
    "        \n",
    "        # Logowanie IoU\n",
    "        self.log(f'{prefix}_iou', iou, on_epoch=True, prog_bar=True)\n",
    "        self.log(f'{prefix}_iou_epoch', iou, on_epoch=True, prog_bar=False)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, 'val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, 'test')\n",
    "\n",
    "\n",
    "    \n",
    "# test\n",
    "batch_size = 2\n",
    "x_test = torch.randn(batch_size, 3, 384, 384)\n",
    "y_test = torch.rand(batch_size, 1, 384, 384)\n",
    "y_test = (y_test > 0.5).float()\n",
    "\n",
    "print(f\"Unikalne wartości w y_test: {torch.unique(y_test)}\")\n",
    "\n",
    "# Test modelu\n",
    "model = UNetSmall(in_ch=3, out_ch=1, dropout=0.1)\n",
    "y_pred = model(x_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {y_pred.shape}\")\n",
    "\n",
    "# Test funkcji straty\n",
    "dice_loss = DiceLoss()\n",
    "loss_value = dice_loss(y_pred, y_test)\n",
    "print(f\"Dice loss: {loss_value:.4f}\")\n",
    "\n",
    "# Test Lightning Module\n",
    "lit_model = LitUNet(in_ch=3, lr=1e-3, dropout=0.1)\n",
    "\n",
    "# Test optymalizatora\n",
    "optimizer = lit_model.configure_optimizers()\n",
    "print(\"Optymalizator i scheduler skonfigurowane poprawnie\")\n",
    "\n",
    "# Test pojedynczego kroku treningowego\n",
    "with torch.no_grad():\n",
    "    loss = lit_model.training_step((x_test, y_test), 0)\n",
    "    print(f\"Training step loss: {loss:.4f}\")\n",
    "\n",
    "# Test MetricsCallback\n",
    "metrics_callback = MetricsCallback()\n",
    "print(\"MetricsCallback zainicjalizowany poprawnie\")\n",
    "\n",
    "print(\"Wszystkie testy zakończone pomyślnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c5b3d",
   "metadata": {},
   "source": [
    "## Zadanie 3 – Trening U-Net: callbacks, wizualizacja metryk i ewaluacja\n",
    "\n",
    "Teraz chcemy wytrenować stworzoną architekturę sieci do segmentacji.\n",
    "\n",
    "1. Stworzenie modelu i callbacków:\n",
    "   - Utwórz instancję klasy sieci.\n",
    "   - Utwórz obiekty `MetricsCallback` – będzie zbierać metryki do wizualizacji.\n",
    "   - Utwórz `EarlyStopping` – ma zatrzymać trening jeśli metryka F1/Dice na zbiorze walidacyjnym nie rośnie przez określoną liczbę epok.\n",
    "   - Utwórz `ModelCheckpoint` – ma zapisywać najlepszy model pod względem metryki F1/Dice dla zbioru walidacyjnego.\n",
    "\n",
    "2. **Trening:**\n",
    "   - Stwórz `Trainer(max_epochs=50, accelerator='auto', devices=1, callbacks=[metrics_callback, early, ckpt], precision=16)`. Określ maksymalną liczbę epok, ustaw `accelerator` jako `'auto'`, przekaż callbacki w `callbacks`. Możesz również sprawdzić jak precyzja numeryczna wpływa na szybkość i wyniki uczenia. Jest ona przekazywana przez argument `precision`. Porównaj wartość 32 i 16 (stosowanie Automatic Mixed Precision).\n",
    "   - Wykonaj trening sieci `trainer.fit`.\n",
    "\n",
    "3. **Wizualizacja przebiegu treningu:**\n",
    "   - Wyciągnij zebrane metryki z `metrics_callback.metrics`. Jest to słownik zawierający nazwy pól jak w wywołaniach metod `self.log` sieci.\n",
    "   - Stwórz 3 wykresy:\n",
    "     -  Train Loss i Val Loss w funkcji epok.\n",
    "     -  Train Dice i Val F1/Dice w funkcji epok.\n",
    "     -  Train IoU i Val IoU w funkcji epok.\n",
    "\n",
    "4. **Test na zbiorze testowym:**\n",
    "   - Wczytaj najlepszy model.\n",
    "   - Sprawdź metryki na zbiorze testowym za pomocą metody `test`.\n",
    "\n",
    "5. **Wizualizacja predykcji:**\n",
    "   - Przełącz model w tryb ewaluacji.\n",
    "   - Pobierz jeden batch z test loadera: `xb, yb = next(iter(dm.test_dataloader()))`.\n",
    "   - Oblicz predykcje dla pobranego batcha wewnątrz bloku `torch.no_grad():`. Dodatkowo obliczone logity przepuść przez sigmoidę, aby otrzymać wartości prawdopodobieństwa.\n",
    "   - Zbinaryzuj predykcje.\n",
    "   - Dla kilku pierwszych przykładów wyświetl w rzędzie:\n",
    "     - Kolumna 1: Oryginalny obraz: `((xb[i].permute(1,2,0).numpy()*0.5)+0.5).clip(0,1)`.\n",
    "     - Kolumna 2: Maska GT: `yb[i,0]`.\n",
    "     - Kolumna 3: Predykcja: `preds[i,0]`.\n",
    "     - Kolumna 4: Obraz oryginalny z nałożoną predykcją (zmień kolor pikseli na czerwony):\n",
    "     `overlay = img.copy(); m = preds[i,0].numpy()>0.5; overlay[m] = [1.0, 0.0, 0.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b058f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c3ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolina/studia/piaom/venv/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/karolina/studia/piaom/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/karolina/studia/piaom/venv/lib/python3.10/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | UNetSmall          | 7.7 M  | train\n",
      "1 | bce_loss  | BCEWithLogitsLoss  | 0      | train\n",
      "2 | dice_loss | DiceLoss           | 0      | train\n",
      "3 | train_iou | BinaryJaccardIndex | 0      | train\n",
      "4 | val_iou   | BinaryJaccardIndex | 0      | train\n",
      "5 | test_iou  | BinaryJaccardIndex | 0      | train\n",
      "6 | train_f1  | BinaryF1Score      | 0      | train\n",
      "7 | val_f1    | BinaryF1Score      | 0      | train\n",
      "8 | test_f1   | BinaryF1Score      | 0      | train\n",
      "---------------------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.812    Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== U-NET TRAINING ===\n",
      "\n",
      "1. Initializing model and callacks...\n",
      "Created model and callbacks.\n",
      "Training...\n",
      "Rozmiary zbiorów: Train: 700, Val: 150, Test: 150\n",
      "Epoch 0:  50%|████▉     | 87/175 [00:28<00:28,  3.05it/s, v_num=9, train_loss_step=1.090, train_dice_step=0.237, train_iou_step=0.134]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved. New best score: 0.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|████▉     | 87/175 [00:32<00:32,  2.68it/s, v_num=9, train_loss_step=1.090, train_dice_step=0.237, train_iou_step=0.134, val_loss=1.120, val_dice=0.304, val_iou=0.186]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 87: 'val_dice' reached 0.30409 (best 0.30409), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=00-val_dice=0.304.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 174/175 [00:48<00:00,  3.55it/s, v_num=9, train_loss_step=1.120, train_dice_step=0.380, train_iou_step=0.234, val_loss=1.120, val_dice=0.304, val_iou=0.186]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.122 >= min_delta = 0.0. New best score: 0.426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 174/175 [00:51<00:00,  3.40it/s, v_num=9, train_loss_step=1.120, train_dice_step=0.380, train_iou_step=0.234, val_loss=1.090, val_dice=0.426, val_iou=0.279]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 174: 'val_dice' reached 0.42642 (best 0.42642), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=00-val_dice=0.426.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|████▉     | 87/175 [00:16<00:17,  5.13it/s, v_num=9, train_loss_step=1.340, train_dice_step=0.101, train_iou_step=0.0533, val_loss=1.090, val_dice=0.397, val_iou=0.257, train_loss_epoch=1.150, train_dice_epoch=0.331, train_iou_epoch=0.209]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 262: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 174/175 [00:31<00:00,  5.48it/s, v_num=9, train_loss_step=0.936, train_dice_step=0.617, train_iou_step=0.446, val_loss=1.090, val_dice=0.397, val_iou=0.257, train_loss_epoch=1.150, train_dice_epoch=0.331, train_iou_epoch=0.209]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.004 >= min_delta = 0.0. New best score: 0.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 174/175 [00:33<00:00,  5.12it/s, v_num=9, train_loss_step=0.936, train_dice_step=0.617, train_iou_step=0.446, val_loss=1.100, val_dice=0.430, val_iou=0.282, train_loss_epoch=1.150, train_dice_epoch=0.331, train_iou_epoch=0.209]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 349: 'val_dice' reached 0.43004 (best 0.43004), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=01-val_dice=0.430.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|████▉     | 87/175 [00:16<00:17,  5.13it/s, v_num=9, train_loss_step=1.130, train_dice_step=0.259, train_iou_step=0.149, val_loss=1.090, val_dice=0.423, val_iou=0.276, train_loss_epoch=1.100, train_dice_epoch=0.348, train_iou_epoch=0.223]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 437: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.060, train_dice_step=0.419, train_iou_step=0.265, val_loss=1.100, val_dice=0.415, val_iou=0.269, train_loss_epoch=1.100, train_dice_epoch=0.348, train_iou_epoch=0.223]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 524: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.130, train_dice_step=0.269, train_iou_step=0.155, val_loss=1.080, val_dice=0.423, val_iou=0.275, train_loss_epoch=1.090, train_dice_epoch=0.363, train_iou_epoch=0.233]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 612: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.230, train_dice_step=0.200, train_iou_step=0.111, val_loss=1.100, val_dice=0.360, val_iou=0.228, train_loss_epoch=1.090, train_dice_epoch=0.363, train_iou_epoch=0.233]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 699: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.090, train_dice_step=0.378, train_iou_step=0.233, val_loss=1.070, val_dice=0.428, val_iou=0.280, train_loss_epoch=1.090, train_dice_epoch=0.393, train_iou_epoch=0.255]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 787: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|█████████▉| 174/175 [00:31<00:00,  5.47it/s, v_num=9, train_loss_step=1.000, train_dice_step=0.331, train_iou_step=0.198, val_loss=1.070, val_dice=0.428, val_iou=0.280, train_loss_epoch=1.090, train_dice_epoch=0.393, train_iou_epoch=0.255] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.015 >= min_delta = 0.0. New best score: 0.445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.000, train_dice_step=0.331, train_iou_step=0.198, val_loss=1.080, val_dice=0.445, val_iou=0.295, train_loss_epoch=1.090, train_dice_epoch=0.393, train_iou_epoch=0.255]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 874: 'val_dice' reached 0.44527 (best 0.44527), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=04-val_dice=0.445.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.120, train_dice_step=0.451, train_iou_step=0.291, val_loss=1.070, val_dice=0.421, val_iou=0.273, train_loss_epoch=1.080, train_dice_epoch=0.406, train_iou_epoch=0.263] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 962: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.120, train_dice_step=0.283, train_iou_step=0.165, val_loss=1.050, val_dice=0.419, val_iou=0.273, train_loss_epoch=1.080, train_dice_epoch=0.406, train_iou_epoch=0.263]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1049: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.160, train_dice_step=0.210, train_iou_step=0.117, val_loss=1.050, val_dice=0.431, val_iou=0.284, train_loss_epoch=1.060, train_dice_epoch=0.424, train_iou_epoch=0.279] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1137: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.909, train_dice_step=0.569, train_iou_step=0.397, val_loss=1.040, val_dice=0.438, val_iou=0.289, train_loss_epoch=1.060, train_dice_epoch=0.424, train_iou_epoch=0.279] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1224: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  50%|████▉     | 87/175 [00:14<00:14,  5.89it/s, v_num=9, train_loss_step=0.942, train_dice_step=0.448, train_iou_step=0.288, val_loss=1.040, val_dice=0.438, val_iou=0.289, train_loss_epoch=1.060, train_dice_epoch=0.408, train_iou_epoch=0.266] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.012 >= min_delta = 0.0. New best score: 0.457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  50%|████▉     | 87/175 [00:17<00:17,  5.12it/s, v_num=9, train_loss_step=0.942, train_dice_step=0.448, train_iou_step=0.288, val_loss=1.020, val_dice=0.457, val_iou=0.306, train_loss_epoch=1.060, train_dice_epoch=0.408, train_iou_epoch=0.266]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1312: 'val_dice' reached 0.45714 (best 0.45714), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=07-val_dice=0.457.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  99%|█████████▉| 174/175 [00:31<00:00,  5.44it/s, v_num=9, train_loss_step=1.070, train_dice_step=0.443, train_iou_step=0.285, val_loss=1.020, val_dice=0.457, val_iou=0.306, train_loss_epoch=1.060, train_dice_epoch=0.408, train_iou_epoch=0.266]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.012 >= min_delta = 0.0. New best score: 0.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  99%|█████████▉| 174/175 [00:34<00:00,  5.09it/s, v_num=9, train_loss_step=1.070, train_dice_step=0.443, train_iou_step=0.285, val_loss=1.030, val_dice=0.470, val_iou=0.316, train_loss_epoch=1.060, train_dice_epoch=0.408, train_iou_epoch=0.266]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1399: 'val_dice' reached 0.46955 (best 0.46955), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=07-val_dice=0.470.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.921, train_dice_step=0.476, train_iou_step=0.312, val_loss=1.010, val_dice=0.448, val_iou=0.298, train_loss_epoch=1.040, train_dice_epoch=0.439, train_iou_epoch=0.291] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1487: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.897, train_dice_step=0.557, train_iou_step=0.386, val_loss=1.010, val_dice=0.444, val_iou=0.294, train_loss_epoch=1.040, train_dice_epoch=0.439, train_iou_epoch=0.291]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1574: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  50%|████▉     | 87/175 [00:17<00:17,  5.11it/s, v_num=9, train_loss_step=0.976, train_dice_step=0.503, train_iou_step=0.336, val_loss=1.010, val_dice=0.456, val_iou=0.304, train_loss_epoch=1.030, train_dice_epoch=0.441, train_iou_epoch=0.293] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1662: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.897, train_dice_step=0.562, train_iou_step=0.391, val_loss=1.010, val_dice=0.455, val_iou=0.303, train_loss_epoch=1.030, train_dice_epoch=0.441, train_iou_epoch=0.293] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1749: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.120, train_dice_step=0.289, train_iou_step=0.169, val_loss=1.010, val_dice=0.457, val_iou=0.305, train_loss_epoch=1.020, train_dice_epoch=0.450, train_iou_epoch=0.299] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1837: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.881, train_dice_step=0.530, train_iou_step=0.360, val_loss=1.010, val_dice=0.460, val_iou=0.308, train_loss_epoch=1.020, train_dice_epoch=0.450, train_iou_epoch=0.299]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1924: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  50%|████▉     | 87/175 [00:14<00:14,  5.89it/s, v_num=9, train_loss_step=0.941, train_dice_step=0.525, train_iou_step=0.356, val_loss=1.010, val_dice=0.460, val_iou=0.308, train_loss_epoch=1.020, train_dice_epoch=0.455, train_iou_epoch=0.304]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.005 >= min_delta = 0.0. New best score: 0.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.941, train_dice_step=0.525, train_iou_step=0.356, val_loss=1.010, val_dice=0.475, val_iou=0.321, train_loss_epoch=1.020, train_dice_epoch=0.455, train_iou_epoch=0.304]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2012: 'val_dice' reached 0.47483 (best 0.47483), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=11-val_dice=0.475.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  99%|█████████▉| 174/175 [00:34<00:00,  5.09it/s, v_num=9, train_loss_step=1.110, train_dice_step=0.470, train_iou_step=0.307, val_loss=1.000, val_dice=0.468, val_iou=0.315, train_loss_epoch=1.020, train_dice_epoch=0.455, train_iou_epoch=0.304] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2099: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  50%|████▉     | 87/175 [00:17<00:17,  5.11it/s, v_num=9, train_loss_step=0.836, train_dice_step=0.714, train_iou_step=0.555, val_loss=1.000, val_dice=0.469, val_iou=0.316, train_loss_epoch=1.020, train_dice_epoch=0.454, train_iou_epoch=0.303] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2187: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  99%|█████████▉| 174/175 [00:34<00:00,  5.10it/s, v_num=9, train_loss_step=1.020, train_dice_step=0.468, train_iou_step=0.306, val_loss=1.000, val_dice=0.465, val_iou=0.313, train_loss_epoch=1.020, train_dice_epoch=0.454, train_iou_epoch=0.303]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2274: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  50%|████▉     | 87/175 [00:14<00:14,  5.88it/s, v_num=9, train_loss_step=0.988, train_dice_step=0.496, train_iou_step=0.329, val_loss=1.000, val_dice=0.465, val_iou=0.313, train_loss_epoch=1.010, train_dice_epoch=0.465, train_iou_epoch=0.312] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.007 >= min_delta = 0.0. New best score: 0.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.988, train_dice_step=0.496, train_iou_step=0.329, val_loss=0.997, val_dice=0.482, val_iou=0.327, train_loss_epoch=1.010, train_dice_epoch=0.465, train_iou_epoch=0.312]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2362: 'val_dice' reached 0.48171 (best 0.48171), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=13-val_dice=0.482.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  99%|█████████▉| 174/175 [00:31<00:00,  5.44it/s, v_num=9, train_loss_step=1.310, train_dice_step=0.181, train_iou_step=0.0996, val_loss=0.997, val_dice=0.482, val_iou=0.327, train_loss_epoch=1.010, train_dice_epoch=0.465, train_iou_epoch=0.312]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.006 >= min_delta = 0.0. New best score: 0.488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  99%|█████████▉| 174/175 [00:34<00:00,  5.09it/s, v_num=9, train_loss_step=1.310, train_dice_step=0.181, train_iou_step=0.0996, val_loss=0.999, val_dice=0.488, val_iou=0.332, train_loss_epoch=1.010, train_dice_epoch=0.465, train_iou_epoch=0.312]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2449: 'val_dice' reached 0.48772 (best 0.48772), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=13-val_dice=0.488.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.130, train_dice_step=0.212, train_iou_step=0.118, val_loss=1.000, val_dice=0.458, val_iou=0.307, train_loss_epoch=1.010, train_dice_epoch=0.466, train_iou_epoch=0.315]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2537: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.914, train_dice_step=0.669, train_iou_step=0.503, val_loss=0.991, val_dice=0.463, val_iou=0.311, train_loss_epoch=1.010, train_dice_epoch=0.466, train_iou_epoch=0.315] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2624: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  50%|████▉     | 87/175 [00:17<00:17,  5.12it/s, v_num=9, train_loss_step=1.010, train_dice_step=0.392, train_iou_step=0.244, val_loss=1.020, val_dice=0.469, val_iou=0.316, train_loss_epoch=1.010, train_dice_epoch=0.452, train_iou_epoch=0.302]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 2712: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.240, train_dice_step=0.197, train_iou_step=0.109, val_loss=0.987, val_dice=0.486, val_iou=0.331, train_loss_epoch=1.010, train_dice_epoch=0.452, train_iou_epoch=0.302]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 2799: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  50%|████▉     | 87/175 [00:17<00:17,  5.12it/s, v_num=9, train_loss_step=1.150, train_dice_step=0.240, train_iou_step=0.136, val_loss=1.050, val_dice=0.478, val_iou=0.324, train_loss_epoch=1.020, train_dice_epoch=0.448, train_iou_epoch=0.300] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 2887: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.897, train_dice_step=0.450, train_iou_step=0.291, val_loss=1.000, val_dice=0.462, val_iou=0.310, train_loss_epoch=1.020, train_dice_epoch=0.448, train_iou_epoch=0.300] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 2974: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  50%|████▉     | 87/175 [00:14<00:14,  5.89it/s, v_num=9, train_loss_step=1.020, train_dice_step=0.364, train_iou_step=0.222, val_loss=1.000, val_dice=0.462, val_iou=0.310, train_loss_epoch=1.020, train_dice_epoch=0.459, train_iou_epoch=0.307]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.004 >= min_delta = 0.0. New best score: 0.491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.020, train_dice_step=0.364, train_iou_step=0.222, val_loss=1.020, val_dice=0.491, val_iou=0.335, train_loss_epoch=1.020, train_dice_epoch=0.459, train_iou_epoch=0.307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3062: 'val_dice' reached 0.49133 (best 0.49133), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=17-val_dice=0.491.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  99%|█████████▉| 174/175 [00:31<00:00,  5.44it/s, v_num=9, train_loss_step=0.811, train_dice_step=0.659, train_iou_step=0.491, val_loss=1.020, val_dice=0.491, val_iou=0.335, train_loss_epoch=1.020, train_dice_epoch=0.459, train_iou_epoch=0.307]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.005 >= min_delta = 0.0. New best score: 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  99%|█████████▉| 174/175 [00:34<00:00,  5.09it/s, v_num=9, train_loss_step=0.811, train_dice_step=0.659, train_iou_step=0.491, val_loss=0.994, val_dice=0.496, val_iou=0.340, train_loss_epoch=1.020, train_dice_epoch=0.459, train_iou_epoch=0.307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3149: 'val_dice' reached 0.49637 (best 0.49637), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=17-val_dice=0.496.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.982, train_dice_step=0.394, train_iou_step=0.245, val_loss=1.130, val_dice=0.451, val_iou=0.300, train_loss_epoch=1.030, train_dice_epoch=0.446, train_iou_epoch=0.298]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3237: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  99%|█████████▉| 174/175 [00:31<00:00,  5.47it/s, v_num=9, train_loss_step=1.030, train_dice_step=0.383, train_iou_step=0.237, val_loss=1.130, val_dice=0.451, val_iou=0.300, train_loss_epoch=1.030, train_dice_epoch=0.446, train_iou_epoch=0.298] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_dice improved by 0.002 >= min_delta = 0.0. New best score: 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.030, train_dice_step=0.383, train_iou_step=0.237, val_loss=0.998, val_dice=0.499, val_iou=0.342, train_loss_epoch=1.030, train_dice_epoch=0.446, train_iou_epoch=0.298]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3324: 'val_dice' reached 0.49878 (best 0.49878), saving model to '/home/karolina/studia/piaom/lab5/lightning_logs/version_9/checkpoints/unet-best-epoch=18-val_dice=0.499.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.832, train_dice_step=0.679, train_iou_step=0.514, val_loss=1.000, val_dice=0.488, val_iou=0.332, train_loss_epoch=1.020, train_dice_epoch=0.461, train_iou_epoch=0.310] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3412: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.060, train_dice_step=0.432, train_iou_step=0.275, val_loss=0.989, val_dice=0.491, val_iou=0.334, train_loss_epoch=1.020, train_dice_epoch=0.461, train_iou_epoch=0.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3499: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.030, train_dice_step=0.452, train_iou_step=0.292, val_loss=1.010, val_dice=0.487, val_iou=0.331, train_loss_epoch=1.010, train_dice_epoch=0.462, train_iou_epoch=0.311]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 3587: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.170, train_dice_step=0.379, train_iou_step=0.234, val_loss=1.060, val_dice=0.478, val_iou=0.323, train_loss_epoch=1.010, train_dice_epoch=0.462, train_iou_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 3674: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=1.140, train_dice_step=0.205, train_iou_step=0.114, val_loss=0.985, val_dice=0.475, val_iou=0.321, train_loss_epoch=1.000, train_dice_epoch=0.476, train_iou_epoch=0.324] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 3762: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.857, train_dice_step=0.516, train_iou_step=0.348, val_loss=1.010, val_dice=0.480, val_iou=0.324, train_loss_epoch=1.000, train_dice_epoch=0.476, train_iou_epoch=0.324]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 3849: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.985, train_dice_step=0.410, train_iou_step=0.258, val_loss=0.974, val_dice=0.487, val_iou=0.331, train_loss_epoch=0.998, train_dice_epoch=0.475, train_iou_epoch=0.322] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 3937: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=1.170, train_dice_step=0.155, train_iou_step=0.0842, val_loss=0.999, val_dice=0.440, val_iou=0.292, train_loss_epoch=0.998, train_dice_epoch=0.475, train_iou_epoch=0.322]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 4024: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  50%|████▉     | 87/175 [00:16<00:17,  5.12it/s, v_num=9, train_loss_step=0.950, train_dice_step=0.504, train_iou_step=0.337, val_loss=0.976, val_dice=0.484, val_iou=0.328, train_loss_epoch=0.986, train_dice_epoch=0.482, train_iou_epoch=0.327]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 4112: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|█████████▉| 174/175 [00:31<00:00,  5.47it/s, v_num=9, train_loss_step=0.672, train_dice_step=0.753, train_iou_step=0.604, val_loss=0.976, val_dice=0.484, val_iou=0.328, train_loss_epoch=0.986, train_dice_epoch=0.482, train_iou_epoch=0.327]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_dice did not improve in the last 10 records. Best score: 0.499. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|█████████▉| 174/175 [00:34<00:00,  5.11it/s, v_num=9, train_loss_step=0.672, train_dice_step=0.753, train_iou_step=0.604, val_loss=0.987, val_dice=0.465, val_iou=0.314, train_loss_epoch=0.986, train_dice_epoch=0.482, train_iou_epoch=0.327]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 4199: 'val_dice' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:  99%|█████████▉| 174/175 [00:34<00:00,  5.09it/s, v_num=9, train_loss_step=0.672, train_dice_step=0.753, train_iou_step=0.604, val_loss=0.987, val_dice=0.465, val_iou=0.314, train_loss_epoch=0.979, train_dice_epoch=0.491, train_iou_epoch=0.336]\n"
     ]
    }
   ],
   "source": [
    "# Zadanie 3 – Trening U-Net: callbacks, wizualizacja metryk i ewaluacja\n",
    "\n",
    "print(\"=== U-NET TRAINING ===\")\n",
    "\n",
    "print(\"\\n1. Initializing model and callacks...\")\n",
    "\n",
    "dm = KvasirDataModule(\n",
    "    img_paths=imgs,\n",
    "    msk_paths=msks,\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    img_size=(384, 384)\n",
    ")\n",
    "\n",
    "model = LitUNet(in_ch=3, lr=1e-3, dropout=0.2)\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_dice',\n",
    "    mode='max',\n",
    "    patience=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',  # Changed from 'val_f1' to 'val_dice'\n",
    "    mode='max',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    filename='unet-best-{epoch:02d}-{val_dice:.3f}',  # Also updated filename\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Created model and callbacks.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[metrics_callback, early_stopping, checkpoint_callback],\n",
    "    precision=16,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.5\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 3. Wizualizacja przebiegu treningu\n",
    "print(\"\\n3. Metrics visualization\")\n",
    "\n",
    "metrics = metrics_callback.metrics\n",
    "\n",
    "if metrics['epoch']:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    axes[0].plot(metrics['epoch'], metrics['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(metrics['epoch'][:len(metrics['val_loss'])], metrics['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoka')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Train and Val Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(metrics['epoch'], metrics['train_dice'], 'b-', label='Train Dice', linewidth=2)\n",
    "    axes[1].plot(metrics['epoch'][:len(metrics['val_dice'])], metrics['val_dice'], 'r-', label='Val Dice', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoka')\n",
    "    axes[1].set_ylabel('Dice/F1 Score')\n",
    "    axes[1].set_title('Train and Val Dice Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(metrics['epoch'], metrics['train_iou'], 'b-', label='Train IoU', linewidth=2)\n",
    "    axes[2].plot(metrics['epoch'][:len(metrics['val_iou'])], metrics['val_iou'], 'r-', label='Val IoU', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoka')\n",
    "    axes[2].set_ylabel('IoU')\n",
    "    axes[2].set_title('Train and Val IoU')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    best_val_f1 = max(metrics['val_dice']) if metrics['val_dice'] else 0\n",
    "    best_val_iou = max(metrics['val_iou']) if metrics['val_iou'] else 0\n",
    "    best_epoch = metrics['epoch'][metrics['val_dice'].index(best_val_f1)] if metrics['val_dice'] else 0\n",
    "    \n",
    "    print(f\"\\nBest val results:\")\n",
    "    print(f\"Epoka: {best_epoch}\")\n",
    "    print(f\"Val Dice/F1: {best_val_f1:.4f}\")\n",
    "    print(f\"Val IoU: {best_val_iou:.4f}\")\n",
    "else:\n",
    "    print(\"No data to visualize...\")\n",
    "\n",
    "\n",
    "print(\"\\n4. Testing...\")\n",
    "\n",
    "if checkpoint_callback.best_model_path:\n",
    "    print(f\"Loading the best model: {checkpoint_callback.best_model_path}\")\n",
    "    best_model = LitUNet.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "else:\n",
    "    print(\"Using latest model...\")\n",
    "    best_model = model\n",
    "\n",
    "test_results = trainer.test(best_model, dm)\n",
    "print(\"Testing done\")\n",
    "\n",
    "\n",
    "print(\"\\n5. Visualizing predictions...\")\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "test_loader = dm.test_dataloader()\n",
    "xb, yb = next(iter(test_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = best_model(xb)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float()\n",
    "\n",
    "print(f\"Visualizing batch: {xb.shape}\")\n",
    "\n",
    "n_examples = min(4, xb.shape[0])\n",
    "\n",
    "fig, axes = plt.subplots(n_examples, 4, figsize=(16, 4 * n_examples))\n",
    "\n",
    "if n_examples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(n_examples):\n",
    "    img = xb[i].permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5) + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    gt_mask = yb[i, 0].numpy()\n",
    "    \n",
    "    pred_mask = preds[i, 0].numpy()\n",
    "    \n",
    "    overlay = img.copy()\n",
    "    m = pred_mask > 0.5\n",
    "    overlay[m] = [1.0, 0.0, 0.0]\n",
    "    \n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f'Obraz {i+1}')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(gt_mask, cmap='gray')\n",
    "    axes[i, 1].set_title(f'GT Maska {i+1}')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(pred_mask, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Predykcja {i+1}')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[i, 3].imshow(overlay)\n",
    "    axes[i, 3].set_title(f'Overlay {i+1}')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Visualizing predictions on test dataset', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n6. Analyzing prediction quality...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_iou = BinaryJaccardIndex()(preds.int(), yb.int())\n",
    "    test_f1 = BinaryF1Score()(preds.int(), yb.int())\n",
    "    \n",
    "    print(f\"Prediction quality on a test batch:\")\n",
    "    print(f\"IoU: {test_iou:.4f}\")\n",
    "    print(f\"F1/Dice: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n7. Checkpoints:\")\n",
    "if checkpoint_callback.best_model_path:\n",
    "    print(f\"Best model: {checkpoint_callback.best_model_path}\")\n",
    "if hasattr(checkpoint_callback, 'last_model_path') and checkpoint_callback.last_model_path:\n",
    "    print(f\"Latest model: {checkpoint_callback.last_model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINISHED TRAINING!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd5cc1",
   "metadata": {},
   "source": [
    "## Zadanie 4 – Transfer Learning: U-Net z pre-trained ResNet Encoder\n",
    "\n",
    "Znowu wykorzystamy transfer learning do zbudowania sieci o większej skuteczności. Zamiast używać całej nauczonej wcześniej sieci wyciągniemy tylko kilka warstw i użyjemy ich w enkoderze modelu U-Net.\n",
    "\n",
    "1. **ResNetEncoder:**\n",
    "   - Zaimplementuj klasę `ResNetEncoder`, która dziedziczy po `nn.Module`.\n",
    "   - W `__init__` przyjmij: `pretrained=True` (czy ładować pretrenowane wagi), `freeze_encoder=False` (czy wczytane wagi mają być zamrożone).\n",
    "   - Wczytaj pretrained ResNet18: `models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)`.\n",
    "   - Wyciągnij warstwy enkodera:\n",
    "     - `resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool` – początkowe warstwy.\n",
    "     - `resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4` – bloki ResNet (64, 128, 256, 512 kanałów).\n",
    "   - Jeśli `freeze_encoder=True`, ustaw wszystkie parametry na `requires_grad=False` wewnątrz bloku `for param in self.parameters():`\n",
    "   - W metodzie `forward(x)` przepuść dane przez warstwy i zwróć 5 tensorów skip connections: `x1` (po conv1), `x2` (po layer1), `x3` (po layer2), `x4` (po layer3), `x5` (po layer4/bottleneck).\n",
    "\n",
    "2. **UNetResNet:**\n",
    "   - Zaimplementuj klasę `UNetResNet`, która dziedziczy po `nn.Module`.\n",
    "   - W `__init__` przyjmij: `pretrained=True`, `freeze_encoder=False`, `dropout`.\n",
    "   - Stwórz encoder: `self.encoder = ResNetEncoder(pretrained, freeze_encoder)`.\n",
    "   - Zbuduj decoder (podobnie jak w poprzednim zadaniu):\n",
    "     - `ConvTranspose2d` upsampling z bottleneck (u1).\n",
    "     - `DoubleConv` pamiętaj o konkatenacji wyjścia z u1 i skip connection z x4.\n",
    "     - `ConvTranspose2d` (u2).\n",
    "     - `DoubleConv` u2 + x3.\n",
    "     - `ConvTranspose2d` (u3).\n",
    "     - `DoubleConv` u3 + x2.\n",
    "     - `ConvTranspose2d` (u4).\n",
    "     - `DoubleConv` u4 + x1.\n",
    "     - `ConvTranspose2d` powrót do orygionalej rozdzielczości.\n",
    "     - `DoubleConv`.\n",
    "   - Warstwa wyjściowa `Conv2d` z jednym kanałem wyjściowym o rozmiarze 1x1.\n",
    "   - W metodzie `forward(x)`:\n",
    "     - Wywołaj encoder: `x1, x2, x3, x4, x5 = self.encoder(x)`.\n",
    "     - Przepuść przez decoder i skip connections.\n",
    "     - Zwróć logity.\n",
    "\n",
    "3. **LitUNetResNet:**\n",
    "   - Zaimplementuj Lightning wrapper analogicznie do `LitUNet`.\n",
    "   - Dodaj metody pomocnicze:\n",
    "     - `freeze_encoder()`: zamraża wagi enkodera (tylko decoder będzie trenowany).\n",
    "     - `unfreeze_encoder()`: odmraża encoder (cała sieć będzie trenowana).\n",
    "\n",
    "4. **Test architektury:**\n",
    "   - Stwórz instancję `UNetResNet`.\n",
    "   - Przepuść losowy tensor przez sieć i sprawdź kształt wyjścia.\n",
    "   - Wyświetl osobno liczbę wszystkich oraz liczbę uczonych parametrów `sum(p.numel() for p in model_tl.parameters() if p.requires_grad)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f928d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b2d4899",
   "metadata": {},
   "source": [
    "## Zadanie 5 – Dwuetapowy Transfer Learning: Frozen Encoder\n",
    "\n",
    "Trening chcemy wykonać podobnie jak w poprzednim ćwiczeniu, czyli zaczynamy od zamrożonych wag z transfer learningu, a potem wykonujemy fine-tuning. Proces treningu jest taki sam jak dla naszej wcześniejszej sieci.\n",
    "\n",
    "**Trening z zamrożonym encoderem:**\n",
    "1. Stwórz obiekt klasy `LitUNetResNet` z pretrenowanymi i zamrożonymi wagami.\n",
    "2. Stwórz callbacki: `MetricsCallback`, `EarlyStopping`, `ModelCheckpoint`.\n",
    "3. Stwórz `Trainer`.\n",
    "4. Wywołaj trening sieci wywołując metodę `fit`\n",
    "5. Wyświetl najlepszy checkpoint i val_dice.\n",
    "6. Sprawdź oraz wyświetl metryki (3 wykresy: loss, dice, iou) podobnie jak we wcześniejszym treningu.\n",
    "7. Wczytaj najlepszy model sieci i wykonaj dla niego testy na zbiorze testowym.\n",
    "8. Wyświetl przykładowe predykcje (4 obrazy × 4 kolumny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f9fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3fb4997",
   "metadata": {},
   "source": [
    "## Zadanie 6 – Dwuetapowy Transfer Learning: Fine-tuning\n",
    "\n",
    "**Fine-tuning całej sieci:**\n",
    "1. Odmroź encoder dla najlepszego modelu z poporzedniej części.\n",
    "2. Ustaw learning rate: `best_stage1.hparams.lr = ` (podczas fine-tuningu zazwyczaj jest mniejszy).\n",
    "3. Zaktualizuj hparam: `best_stage1.hparams.freeze_encoder = False`.\n",
    "4. Stwórz nowe callbacki: `MetricsCallback`, `EarlyStopping`, `ModelCheckpoint`.\n",
    "5. Stwórz nowy `Trainer`.\n",
    "6. Wykonaj trening wywołując metodę `fit`.\n",
    "7. Sprawdź oraz wyświetl metryki (3 wykresy: loss, dice, iou) podobnie jak we wcześniejszym treningu.\n",
    "8. Wczytaj najlepszy model sieci i wykonaj dla niego testy na zbiorze testowym.\n",
    "9. Wyświetl przykładowe predykcje (4 obrazy × 4 kolumny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9189e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c28eac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie i wnioski\n",
    "\n",
    "Porównaj wyniki przeprowadzonych treningów sieci. Weź pod uwagę liczbę trenowanych parametrów.\n",
    "Odpowiedz również krótko na poniższe pytania. \n",
    "\n",
    "1. Czym segmentacja różni się od zadania klasyfikacji?\n",
    "\n",
    "2. Czym jest architektura U-Net? Czym ona się charakteryzuje?\n",
    "\n",
    "3. Czym jest funkcja straty DiceLoss?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
